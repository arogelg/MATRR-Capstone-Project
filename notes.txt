This monkeys are in the master and monkey datasets but in the master they dont have an open access row: 

10255,
10352,
10353,
10354,
10355,
10356,
10357,
10358,
10359

(WE TALKED ABOUT THIS MONKEYS BEING POSSIBLE INCOMPLETE PROJECT SUBJECTS BUT THE CLIENT EMAILED BACK AND SAID THEY ARENT)

For terms of consistency and simplicity given our short time and in thinking of getting a working product
for this end of spring we decided to keep the clean data eliminating the 9 monkeys in the master as they have very incosistent
row values that we will probably come back to check later and ask our client their position on this approach




Only 119 monkeys are common between the monkey and master dataset, meaning our before and after was done with just 119 monkeys as we only had the drinking category of those



In the monkey dataset, I found this 10 monkeys which had their id but they had no records at all, just their IDs: 

'10033', '10107', '10108', '10109', '10110', '10111', '10112', '10113', '10114', '10159'

Out of all of this, only 10033 is in the Master, so we only reduced our common monkeys to 118


We also found 486 rows in the monkey dataset without start and end time

295 rows have 6 missing columns, 6 have 7 missing columns, and then 82 have 11 missing columns, the last 3 have 12. 

So we for sure will drop the ones that have 10 or more missing columns but well ask the client how to proceed with the rest

Thankfully out of those 401 none belong to a single monkey meaning that all monkeys have AT LST 1 recorded time. The list looks like this for the missing values for monkey ID:

mky_id  # of NoTimeRecordedRows
10005    38
10006    39
10007    38
10008    39
10010    39
10011    39
10012    39
10013    38
10014    39
10015    40
10024     1
10025     1
10052     1
10069     1
10070     1
10078     1
10079     1
10080     1
10081     1
10102     4

For the ones above 30 we need to check how many valid recorded times each of those have and see if we have relevant data from them at all, else, we may still need to consider dropping them


I consider than more than 10% of their rows missing could potentially be problematic but given our lack of monkey data, lets ask our client, this is the final record: 

Summary of missing time data for monkeys with more than 30 missing rows:
        no_time_rows  total_rows  percentage_missing
mky_id                                              
10005             38         527                7.21
10006             39         445                8.76
10007             38         509                7.47
10008             39         616                6.33
10010             39         295               13.22
10011             39         226               17.26
10012             39         299               13.04
10013             38         389                9.77
10014             39         382               10.21
10015             40         344               11.63

If decided to, we would need to drop: 10015, 10014, 10012, 10011, 10010

(NOTE: None of this is in the master dataset anyway)




mtd_etoh_intake, mtd_veh_intake, and mtd_total_pellets have minimum values of -1.0. SO ASK if this is normal or if we should change those values to NaNs


**** Most of our problems in the monkey dataset where solved or mainly reduced to be insignificant when using only the monkeys that are part of open access 1 or that have explicit open access in the master dataset